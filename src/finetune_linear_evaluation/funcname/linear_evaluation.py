import os
import sys
import torch
import json
import time
import wandb
import argparse
import numpy as np
import torch.nn as nn
import torch.nn.functional as F
from sklearn.metrics import f1_score
from funcnamecls import FuncNamecls
from symlmdataset import SymlmDataset
sys.path.insert(0,"src/utils")

from save_model import save_model
from yaml_config_hook import yaml_config_hook
from metrics import results_to_excel

key_to_pop = ['opti']

def get_top_prob(prediction, probability, prob_threshold=0.5):
    preds = prediction.split(' ')
    # preds = preds[:topK]
    probs = probability.replace('[', '').replace(']', '')
    probs = probs.split(' ')
    res = []
    for i, prob in enumerate(probs):
        prob = float(prob)
        if prob >= prob_threshold:
            res.append(preds[i])
    if len(res) == 0:
        res = preds[:1]
    return ' '.join(res)

def split_words(line):
    return line.lower().strip().split(' ')

def calculate_results(true_positive, false_positive, false_negative, total_target):
    # avoid dev by 0
    if true_positive + false_positive == 0:
        return 0, 0, 0
    precision = true_positive / (true_positive + false_positive)
    recall = true_positive / (true_positive + false_negative)
    acc = true_positive / total_target
    if precision + recall > 0:
        f1 = 2 * precision * recall / (precision + recall)
    else:
        f1 = 0
    return precision, recall, f1, acc

def get_correct_predictions_word_cluster(target, prediction, word_cluster):
    """
    Calculate predictions based on word cluster generated by CodeWordNet.
    """
    true_positive, false_positive, false_negative = 0, 0, 0
    replacement = dict()
    skip = set()
    for j, p in enumerate(prediction):
        if p in target:
            skip.add(j)
    for i, t in enumerate(target):
        for j, p in enumerate(prediction):
            if t != p and j not in replacement and j not in skip:
                if t in word_cluster and p in word_cluster:
                    t_cluster = word_cluster[t]
                    p_cluster = word_cluster[p]
                    t_cluster, p_cluster = set(t_cluster), set(p_cluster)
                    if len(t_cluster.intersection(p_cluster)) > 0:
                        replacement[j] = t
    for k, v in replacement.items():
        prediction[k] = v
    if target == prediction:
        true_positive = len(target)
    else:
        target = set(target)
        prediction = set(prediction)

        true_positive += len(target.intersection(prediction))
        false_negative += len(target.difference(prediction))
        false_positive += len(prediction.difference(target))

    total_target = len(target)

    return true_positive, false_positive, false_negative, total_target

def train(args, loader, model, criterion, optimizer):
    loss_epoch = 0
    true_positve_epoch = 0
    false_positve_epoch = 0
    false_negative_epoch = 0
    model.train()

    for step, (x, y) in enumerate(loader):
        optimizer.zero_grad()

        x_info = {}
        for key in key_to_pop:
            x_info[key] = x.pop(key)

        x = {feature: value.cuda(non_blocking=True) for feature, value in x.items()}
        y = y.to(args.device)

        logits = model(x)
        loss = criterion(logits, y)           

        preds = F.sigmoid(logits)
        preds = preds > 0.5
        preds = preds.to(torch.int32)

        true_positve = ((y == 1) * (preds == 1)).sum() 
        false_positve = ((y == 0) * (preds == 1)).sum() 
        false_negative = ((y == 1) * (preds == 0)).sum() 

        loss.backward()
        optimizer.step()

        loss_epoch += loss.item()
        true_positve_epoch += true_positve.item()
        false_positve_epoch += false_positve.item()
        false_negative_epoch += false_negative.item()

        if step % args.show_step == 0:
            print(
                 f"Train Step [{step}/{len(loader)}]\t Loss: {loss.item()}\t TP:{true_positve.item()}\t FP:{false_positve.item()}\t FN:{false_negative.item()}"
             )

    return loss_epoch, true_positve_epoch, false_positve_epoch, false_negative_epoch


def test(args, loader, model, word_cluster):
    model.eval()
    total_inference_time = 0
    total_samples = 0

    for step, (x, y) in enumerate(loader):
        model.zero_grad()

        x_info = {}
        for key in key_to_pop:
            x_info[key] = x.pop(key)

        x = {feature: value.cuda(non_blocking=True) for feature, value in x.items()}
        y = y.to(args.device)

        total_samples += y.shape[0]

        with torch.no_grad():
            start_time = time.time()
            logits = model(x)
            
            targets = [[idx + 4 for idx, target in enumerate(sublist) if target == 1] for sublist in y]
            preds = torch.sigmoid(logits)
            preds_indices = torch.topk(preds, k=args.topk).indices + 4
            preds_values = torch.topk(preds, k=args.topk).values
            preds_string = [' '.join(args.label_tokenizer.convert_ids_to_tokens(indices_row.cpu().detach().tolist())) for indices_row in preds_indices]
            targets_string = [' '.join(args.label_tokenizer.convert_ids_to_tokens(targets_row)) for targets_row in targets]

            archs = args.arch_tokenizer.convert_ids_to_tokens(x['arch'][:, 0])
            optis = x_info['opti']

            for i, (pred, arch, opti) in enumerate(zip(preds_string, archs, optis)):
                with open(args.evaluation_file.format(arch, opti), 'a+') as f:
                    print(f"{targets_string[i]},{pred}," + np.array2string(preds_values[i].cpu().detach().numpy(), formatter={'float_kind':lambda x: "%.4f" % x}), file=f)

            if step % args.show_step == 0:
                print(
                    f"Test Step [{step}/{len(loader)}]\t"
                )

            end_time = time.time()
            total_inference_time += (end_time - start_time)

    metrics = {}
    for architecture in args.architecture_list:
        for optimization in args.optimization_list:
            start_time = time.time()
            true_positive, false_positive, false_negative, total_target = 0, 0, 0, 0
            total = 0
            targets = []
            predictions = []
            threshold = args.prob_threshold
            with open(args.evaluation_file.format(architecture, optimization), 'r') as f:
                for i, line in enumerate(f):
                    total += 1
                    line = line.strip('\n')
                    lines = line.split(',')
                    lines[1] = get_top_prob(lines[1], lines[2], prob_threshold=threshold)
                    assert isinstance(lines[1], str) and len(lines[1]) > 0, "Don't give empty prediction"
                    targets.append(lines[0])
                    predictions.append(lines[1])
                    target = split_words(lines[0])
                    prediction = split_words(lines[1])
                    tp, fp, fn, tt = get_correct_predictions_word_cluster(target, prediction, word_cluster)
                    true_positive += tp
                    false_positive += fp
                    false_negative += fn
                    total_target += tt
            precision, recall, f1, acc = calculate_results(true_positive, false_positive, false_negative, total_target)
            metrics[architecture, optimization] = {'Average': {'Precision':precision, 'Recall':recall, 'F1 Score': f1, 'Total':total, 'Accuracy': acc}}
            end_time = time.time()
            total_inference_time += (end_time - start_time)

    average_inference_time_per_sample = total_inference_time / total_samples

    # clear content of evaluation file
    for architecture in args.architecture_list:
        for optimization in args.optimization_list:
            with open(args.evaluation_file.format(architecture, optimization), 'w') as f:
                pass

    return metrics, average_inference_time_per_sample


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="SimCLR")
    config = yaml_config_hook("src/0_environment_setup/config.yaml")
    for k, v in config.items():
        parser.add_argument(f"--{k}", default=v, type=type(v))

    args = parser.parse_args()
    args.prob_threshold = 0.3
    args.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    data_size = None
    if ('95_5' in args.model_path) or ('6_reload_then_finetune' in args.model_path):
        assert args.model_path.split('/')[-1] != '95_5', 'need data size'
        data_size, dataset_split, model_size  = args.model_path.split('/')[-1], args.model_path.split('/')[-2], args.model_path.split('/')[-3]
    else:
        assert 'per' not in args.model_path.split('/')[-1], 'no need data size'
        dataset_split, model_size  = args.model_path.split('/')[-1], args.model_path.split('/')[-2]

    if args.dataset_arch == 'x86':
        args.dataset_pt = f'workdir/4_prepare_finetune_dataset/x64/funcname/{dataset_split}'
        projectname = f"GBME_funcname_{dataset_split}_{args.backbone.lower()}_{model_size}_x64"
    elif args.dataset_arch == 'all':
        args.dataset_pt = f'workdir/4_prepare_finetune_dataset/all_architecture/funcname/{dataset_split}'
        projectname = f"GBME_funcname_{dataset_split}_{args.backbone.lower()}_{model_size}_allarch"
    else:
        raise Exception('wrong dataset')

    if args.backbone.lower() == 'roberta':
        if model_size == '1GB':
            assert (args.projection_dim == 768) and (args.layers == 12), 'wrong model configuration'
        elif model_size == '512MB':
            assert (args.projection_dim == 480) and (args.layers == 12), 'wrong model configuration'
        elif model_size == '256MB':
            assert (args.projection_dim == 264) and (args.layers == 12), 'wrong model configuration'
        else:
            raise NotImplementedError
    elif args.backbone.lower() == 'malconv2':
        if model_size == '100MB':
            assert (args.projection_dim == 320) and (args.layers == 12), 'wrong model configuration'
        else:
            raise NotImplementedError
    else:
        raise NotImplementedError

    if args.dataset_arch == 'x86':
        args.architecture_list = ['x86_64']
    elif args.dataset_arch == 'all':
        args.architecture_list = ['x86_32','x86_64','arm_32','mipseb_32']
    else:
        raise NotImplementedError
    
    args.optimization_list = ['O0', 'O1', 'O2', 'O3']
    args.topk = 8

    finetune_from = 'instbound'
    pretrain_split = '95_5'

    name_list = ['funcname', args.backbone.lower()]
    if data_size is not None:
        if '95_5' in args.dataset_pt:
            args.dataset_pt = os.path.join(args.dataset_pt, data_size)
        name_list.append('data_size')
        name_list.append(data_size)
    print(f"dataset path is : {args.dataset_pt}")

    if '6_reload_then_finetune' in args.model_path:
        name_list.insert(1, 'finetuned')
        name_list.append('pretraintask')
        name_list.append(finetune_from)
        name_list.append('pretraindatasplit')
        name_list.append(pretrain_split)
        name_list.append('checkpoint')
        name_list.append(str(args.epoch_num))
    elif '5_finetune_linear_evaluation' in args.model_path:
        if '95_5' not in args.model_path:
            if not args.MLM_pretrain:
                name_list.insert(1, 'no_mlm_pretrained')
                args.model_path = os.path.join(args.model_path, 'no_mlm')
            else:
                args.model_path = os.path.join(args.model_path, 'with_mlm')

    args.label_tokenizer_pt = os.path.join(args.dataset_pt, 'tokenizer.json')
    args.evaluation_file = os.path.join(args.model_path,'evaluation_input_{}_{}.txt')
    
    os.environ['WANDB_DIR'] = args.model_path
    wandb.init(project=projectname, entity='lhxxh', name='_'.join(name_list))
    wandb.config.batch_size = args.logistic_batch_size
    wandb.config.learning_rate = 1e-5

    # clear content of evaluation file
    for architecture in args.architecture_list:
        for optimization in args.optimization_list:
            with open(args.evaluation_file.format(architecture, optimization), 'w') as f:
                pass

    with open("src/5_finetune_linear_evaluation/funcname/word_cluster.json", 'r') as f:
        word_cluster = json.load(f)

    print('Supervised finetune arguments')
    for k, v in vars(args).items():
        print(f'{k} : {v}')

    train_dataset = SymlmDataset(args, 'train')
    test_dataset = SymlmDataset(args, 'test')

    train_loader = torch.utils.data.DataLoader(
        train_dataset,
        batch_size=args.logistic_batch_size,
        shuffle=True,   
        drop_last=True,
        num_workers=args.workers,
    )

    test_loader = torch.utils.data.DataLoader(
        test_dataset,
        batch_size=args.logistic_batch_size,
        shuffle=False,
        drop_last=True,
        num_workers=args.workers,
    )

    # load pre-trained model from checkpoint
    assert 'funcname' in args.model_path, 'dataset and path not compatible'
    if not args.reload:
        assert '5_finetune_linear_evaluation' in args.model_path, 'incorrect model path'
    if args.backbone == 'RoBerta':
        assert 'roberta' in args.model_path, 'backbone and path not compatible'
    elif args.backbone == 'Longformer':
        assert 'longformer' in args.model_path, 'backbone and path not compatible'
    elif args.backbone == 'MalConv2':
        assert 'malconv2' in args.model_path, 'backbone and path not compatible'
    else:
        raise NotImplementedError

    model = FuncNamecls(args)
    if args.reload:
        print('Reloading...')
        if '3_pretrain_contrastive_learning' in args.model_path:
            saved_model_pt = os.path.join(args.model_path, "checkpoint_{}.tar".format(args.epoch_num))
            checkpoint = torch.load(saved_model_pt)
            print(f'Reload from pretraining at {saved_model_pt}.....')
            model.load_state_dict(checkpoint['model_state_dict'])
        elif '5_finetune_linear_evaluation' in args.model_path:
            saved_model_pt = os.path.join(args.model_path, "checkpoint_{}.tar".format(args.epoch_num))
            checkpoint = torch.load(saved_model_pt)
            print(f'Continue finetuning at {saved_model_pt}.....')
            args.logistic_start_epoch = checkpoint['epoch']
            assert args.logistic_start_epoch < args.logistic_epochs, 'invalid logistic_start_epoch'
            model.load_state_dict(checkpoint['model_state_dict'])
        elif '6_reload_then_finetune' in args.model_path:
            saved_model_pt = os.path.join(f'workdir/5_finetune_linear_evaluation/x64/{finetune_from}',args.backbone.lower(),model_size,f'{pretrain_split}',data_size,f'checkpoint_{args.epoch_num}.tar')
            model_state_dict = model.state_dict()
            checkpoint_state_dict = torch.load(saved_model_pt)['model_state_dict']
            # For malconv
            checkpoint_state_dict.pop('out_proj.weight', None)
            checkpoint_state_dict.pop('out_proj.bias', None)
            # For roberta
            checkpoint_state_dict.pop('model.classifier.out_proj.weight', None)
            checkpoint_state_dict.pop('model.classifier.out_proj.bias', None)
            print(f'[Finetune] from previous task at {saved_model_pt} ...') 
            common_keys = model_state_dict.keys() & checkpoint_state_dict.keys()
            missing_keys = model_state_dict.keys() - checkpoint_state_dict.keys()
            unexpected_keys = checkpoint_state_dict.keys() - model_state_dict.keys()
            print("Common keys:", common_keys)
            print("Missing keys in checkpoint:", missing_keys)
            print("Unexpected keys in checkpoint:", unexpected_keys)
            model.load_state_dict(checkpoint_state_dict, strict=False)
        else:
            raise Exception('Invalid path')
    else:
        if args.MLM_pretrain:
            if '95_5' in args.model_path:
                saved_model_pt = os.path.join(f'workdir/5_finetune_linear_evaluation/x64/{finetune_from}',args.backbone.lower(),model_size,f'{pretrain_split}',data_size,f'checkpoint_{args.epoch_num}.tar')
                print(f'[Multi-stage Transfer] from previous task at {saved_model_pt} ...') 
            else:
                saved_model_pt = os.path.join(f'workdir/4.5_mlm_task/x64',args.backbone.lower(),model_size,f'{pretrain_split}','5k',f'checkpoint_{args.MLM_pretrain_baseline_epoch}.tar')
                print(f'[Baseline] MLM pretrain path at {saved_model_pt} ...')
            model_state_dict = model.state_dict()
            checkpoint_state_dict = torch.load(saved_model_pt)['model_state_dict']
            # For malconv
            checkpoint_state_dict.pop('classifier.weight', None)
            checkpoint_state_dict.pop('classifier.bias', None)
            # For roberta
            checkpoint_state_dict.pop('model.classifier.weight', None)
            checkpoint_state_dict.pop('model.classifier.bias', None)      
            common_keys = model_state_dict.keys() & checkpoint_state_dict.keys()
            missing_keys = model_state_dict.keys() - checkpoint_state_dict.keys()
            unexpected_keys = checkpoint_state_dict.keys() - model_state_dict.keys()
            print("Common keys:", common_keys)
            print("Missing keys in checkpoint:", missing_keys)
            print("Unexpected keys in checkpoint:", unexpected_keys)
            model.load_state_dict(checkpoint_state_dict, strict=False)

    if torch.cuda.device_count() > 1:
        print(f'Use {torch.cuda.device_count()} GPUs!')
        model = nn.DataParallel(model)

    model = model.to(args.device)

    optimizer = torch.optim.Adam(model.parameters(), lr=1e-5, weight_decay = args.weight_decay)
    if args.reload and '5_finetune_linear_evaluation' in args.model_path:
        print('Reuse previous optimizer..')
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    criterion = torch.nn.BCEWithLogitsLoss(reduction='sum')

    print('Start finetuning ...')
    args.current_epoch = args.logistic_start_epoch 

    # Early stopping parameters
    best_f1_score = 0.0
    best_f1_epoch = 0
    early_stopping_counter = 0
    early_stopping_patience = args.early_stopping_patience

    for epoch in range(args.logistic_start_epoch, args.logistic_epochs):
        loss_epoch, true_positve_epoch, false_positve_epoch, false_negative_epoch = train(
            args, train_loader, model, criterion, optimizer
        )
        print(
            f"Epoch [{epoch}/{args.logistic_epochs}]\t Loss: {loss_epoch / len(train_loader)}\t TP:{true_positve_epoch / len(train_loader)}\t FP:{false_positve_epoch / len(train_loader)}\t FN:{false_negative_epoch / len(train_loader)}"
        )
        args.current_epoch += 1

        if epoch % args.save_epoch == 0:
            save_model(args, model, optimizer)

        if epoch % args.validation_epoch == 0:
            print("Start Validation ...")
            metrics, average_inference_time = test(
                args, test_loader, model, word_cluster
            )

            '''
            for elements, reports in metrics.items():
                print(f"[Configuration: {elements} |{reports['Total']}| ]\t Precision: {reports['Precision']}\t Recall: {reports['Recall']}\t F1 Score: {reports['F1']}\t Accuracy:{reports['Accuracy']}")
            '''
            results_to_excel(metrics, os.path.join(args.model_path, 'output.xlsx'), show_class=False)

            average_f1_score = sum(reports['Average']['F1 Score'] for elements, reports in metrics.items()) / len(metrics)

            # Early stopping check
            if average_f1_score >= best_f1_score:
                best_f1_score = average_f1_score
                best_f1_epoch = args.current_epoch
                early_stopping_counter = 0
            else:
                early_stopping_counter += 1

            wandb.log({
                'epoch': args.current_epoch,
                'f1_score': average_f1_score,
                'inference_time': average_inference_time,
                })

            # Check if early stopping is triggered
            if early_stopping_counter >= early_stopping_patience:
                print(f"Early stopping triggered. Stopping training at epoch {args.current_epoch}.")
                break 

    wandb.log({
        'best_f1_score': best_f1_score,
        'best_f1_epoch': best_f1_epoch
        })

    save_model(args, model, optimizer)
    wandb.finish()

